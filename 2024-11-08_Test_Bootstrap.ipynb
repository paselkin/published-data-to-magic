{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmagpy\n",
    "import pmagpy.pmagplotlib as pmagplotlib\n",
    "import pmagpy.ipmag as ipmag\n",
    "import pmagpy.pmag as pmag\n",
    "import pmagpy.contribution_builder as cb\n",
    "from pmagpy import convert_2_magic as convert\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import glob as glob\n",
    "import shutil\n",
    "import math\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = Path(os.getcwd())\n",
    "data_folder = home/'data'\n",
    "magic_folder = home/'magic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vgps(sites_df):\n",
    "    # Outputs dataframe with columns: vgp_lat, vgp_lon, vgp_dp, vgp_dm\n",
    "    data=sites_df[['dir_dec','dir_inc','dir_alpha95','lat','lon']].to_numpy()\n",
    "    vgps=np.array(pmag.dia_vgp(data)) \n",
    "    vgps=vgps.transpose()\n",
    "    return(pd.DataFrame(vgps,columns=['vgp_lon','vgp_lat','vgp_dp','vgp_dm'],index=sites_df.index))\n",
    "\n",
    "def to_di_block(df):\n",
    "    df['magnitude']=1.0 # Add this in because Pmagpy assumes that vectors in di_blocks have a magnitude\n",
    "    di_block = df[['dir_dec','dir_inc','magnitude']].apply(lambda x: x.to_list(),axis=1).to_list()\n",
    "    return(di_block)\n",
    "\n",
    "def dataframe_flip(dir_df, how='norm_nh'):\n",
    "    \"\"\"\n",
    "    Flips directions in dataframe given assumptions listed.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_df : Pandas DataFrame with columns:\n",
    "        dir_dec : mean declination\n",
    "        dir_inc : mean inclination\n",
    "        dir_n : number of data points in mean\n",
    "        dir_k : Fisher k statistic for mean\n",
    "    how : assumptions to use when flipping polarity of input directions\n",
    "        'norm_nh' : flip so all directions are aligned with normal assuming northern hemisphere (N/down)\n",
    "        'norm_sh' : flip so all directions are aligned with normal assuming southern hemisphere (S/down)\n",
    "        'rev_nh' : flip so all directions are aligned with reversed assuming northern hemisphere (S/up)\n",
    "        'rev_sh' : flip so all directions are aligned with reversed assuming southern hemisphere (N/up)\n",
    "        'trim_norm_nh', 'trim_norm_sh', etc. : exclude directions that are >90 degrees normal/reversed in specified hemisphere\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    boot_results: Pandas DataFrame with columns:\n",
    "            dir_dec : bootstrapped median declination value\n",
    "            dir_inc : bootstrapped median inclination value\n",
    "            dir_k : bootstrapped median kappa value\n",
    "            dir_n : number of rows in original dataframe\n",
    "            dir_alpha95: A95 associated with median kappa\n",
    "    \"\"\"\n",
    "    # NOTE: THIS IS A PLACEHOLDER FOR RACHEL'S FUNCTION\n",
    "    if how!='norm_nh':\n",
    "        print (\"Unimplemented Command Error.\")\n",
    "    return(dir_df)\n",
    "    \n",
    "\n",
    "def dataframe_average(dir_df, nb=500, column_map=None, flip=None, return_distribution=False):\n",
    "    \"\"\"\n",
    "    Performs a bootstrap for direction DataFrame with parametric bootstrap,\n",
    "    providing bootstrap kappa parameter\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_df : Pandas DataFrame with columns:\n",
    "        dir_dec : mean declination\n",
    "        dir_inc : mean inclination\n",
    "        dir_n : number of data points in mean\n",
    "        dir_k : Fisher k statistic for mean\n",
    "    nb : number of bootstraps, default is 500; if np = 0, do not bootstrap (instead, take Fisher average of input data)\n",
    "    flip : whether/how to flip polarity of input directions\n",
    "        None : Do not flip (assumes all magnetization vectors are normal) - Default\n",
    "        'norm_nh' : flip so all directions are aligned with normal assuming northern hemisphere (N/down)\n",
    "        'norm_sh' : flip so all directions are aligned with normal assuming southern hemisphere (S/down)\n",
    "        'rev_nh' : flip so all directions are aligned with reversed assuming northern hemisphere (S/up)\n",
    "        'rev_sh' : flip so all directions are aligned with reversed assuming southern hemisphere (N/up)\n",
    "        'trim_norm_nh', 'trim_norm_sh', etc. : exclude directions that are >90 degrees normal/reversed in specified hemisphere\n",
    "    return_distribution : return DataFrame of dec, inc, kappa values\n",
    "            (default: False)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    if return_distribution is False:\n",
    "        boot_results: Pandas DataFrame with columns:\n",
    "            dir_dec : bootstrapped median declination value\n",
    "            dir_inc : bootstrapped median inclination value\n",
    "            dir_k : bootstrapped median kappa value\n",
    "            dir_n : number of rows in original dataframe\n",
    "            dir_alpha95: A95 associated with median kappa\n",
    "    if return_distribution is True:\n",
    "        boot_results: Pandas DataFrame with all bootstrapped values of columns above\n",
    "    \n",
    "  \n",
    "    \"\"\"\n",
    "    dir_df=dir_df.dropna()\n",
    "    N = dir_df.shape[0] # Note: N counts only non-NaN elements\n",
    "    if N>1:\n",
    "        if column_map is not None:\n",
    "            dir_df=dir_df.rename(columns=column_map)\n",
    "        if flip is not None:\n",
    "            dir_df=dataframe_flip(dir_df, how=flip)\n",
    "        all_boot_results = pd.DataFrame(np.zeros((nb,5)),columns=['dir_dec','dir_inc','dir_n','dir_k','dir_alpha95'])\n",
    "        if (nb>0):\n",
    "            for k in tqdm(range(nb)):\n",
    "                boot_di=dir_df.apply(lambda x: np.array(list(ipmag.fishrot(k=x['dir_k'],n=int(x['dir_n']),dec=x['dir_dec'],inc=x['dir_inc'],di_block=False))).T,axis=1).explode().apply(lambda x: pd.Series({'dir_dec':x[0],'dir_inc':x[1]}))\n",
    "                all_boot_results.iloc[k,:] = pd.Series(pmag.dir_df_fisher_mean(boot_di)).drop(index=['csd','r']).rename(index={'alpha95':'dir_alpha95',\n",
    "                                          'dec':'dir_dec',\n",
    "                                          'inc':'dir_inc',\n",
    "                                          'k':'dir_k',\n",
    "                                          'n':'dir_n'})\n",
    "\n",
    "            b = 20.**(1./(N -1.))-1.\n",
    "            if return_distribution:\n",
    "                results = all_boot_results\n",
    "                results['a']=1-b*(N-1.)/((N*(results['dir_k']-1.))-1.)\n",
    "                results['dir_n']=N\n",
    "                results['dir_alpha95']=np.degrees(np.arccos(results['a']))\n",
    "                results['result_type']='a'\n",
    "            else:\n",
    "                fisher_avg=pd.DataFrame([pmag.dir_df_fisher_mean(all_boot_results)]).rename(columns={'alpha95':'dir_alpha95',\n",
    "                                          'dec':'dir_dec',\n",
    "                                          'inc':'dir_inc',\n",
    "                                          'k':'dir_k',\n",
    "                                          'n':'dir_n'})\n",
    "                fisher_avg['dir_k'] = np.median(all_boot_results['dir_k'].to_numpy())\n",
    "                fisher_avg['dir_n'] = N\n",
    "                a=1-b*(N-1.)/((N*(fisher_avg['dir_k']-1.))-1.)\n",
    "                fisher_avg['dir_alpha95'] = np.degrees(np.arccos(a))\n",
    "                results=fisher_avg[['dir_dec','dir_inc','dir_k','dir_n','dir_alpha95']]\n",
    "                results['result_type']='a'\n",
    "        else:\n",
    "            fisher_avg=pd.DataFrame([pmag.dir_df_fisher_mean(dir_df)]).rename(columns={'alpha95':'dir_alpha95',\n",
    "                                      'dec':'dir_dec',\n",
    "                                      'inc':'dir_inc',\n",
    "                                      'k':'dir_k',\n",
    "                                      'n':'dir_n'})\n",
    "            results=fisher_avg[['dir_dec','dir_inc','dir_k','dir_n','dir_alpha95']]\n",
    "            results['result_type']='a'\n",
    "    else:\n",
    "        results=dir_df[['dir_dec','dir_inc','dir_k','dir_alpha95']]\n",
    "        results['dir_n']=1\n",
    "        results['result_type']='i'\n",
    "    if column_map is not None:\n",
    "        column_map_r=dict(zip(column_map.values(),column_map.keys()))\n",
    "        results=results.rename(columns=column_map_r)\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Dataset: Letts et al. 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Letts et al (2009)\n",
    "#%%capture sites_conversion_log\n",
    "# Capture output to sites_conversion_log\n",
    "\n",
    "## Convert Sites XLSX Files to MagIC format\n",
    "\n",
    "# In this XLSX file, here is how I interpret the mapping of relevant columns to the MagIC 3.0 data model (PAS 8/16/2024): \n",
    "#  \n",
    "#\n",
    "#  Region: sites.location (Note: there are also 'locations' for each )\n",
    "#  DecLong: sites.lon\n",
    "#  DecLat: sites.lat\n",
    "#  n (col. L): sites.dir_n_total_samples (Note: as per conversation with Jeff and Rachel 10/15/2024, this may not be correct!)\n",
    "#  Full Site: sites.site (had to add this column to remove ambiguities in site numbers)\n",
    "#  GDec: sites.dir_dec\n",
    "#  GInc: sites.dir_inc\n",
    "#  Strike: sites.bed_dip_direction (=(strike+90.)%360.)\n",
    "#  Dip: sites.bed_dip_direction\n",
    "#  n (col. W): sites.dir_n_samples (Note: as per conversation with Jeff and Rachel 10/15/2024, this may not be correct!)\n",
    "#  a95 (col. X): sites.dir_alpha95\n",
    "#  kappa: sites.dir_k\n",
    "#  Lithology: sites.lithologies\n",
    "#  \n",
    "# Assumptions:\n",
    "#\n",
    "#  sites.formation = \"Bushveld Complex\"\n",
    "#  sites.geologic_classes = \"Igenous:Intrusive\"\n",
    "#  sites.geologic_types = \"Layered Inntrusion\"\n",
    "#  sites.result_type = \"i\"\n",
    "#  sites.method_codes = \"DE-BFL:DE-K:LP-DIR-AF:LP-DIR-T\"\n",
    "#  sites.citations = \"10.1111/j.1365-246X.2009.04346.x\"\n",
    "#  sites.age = 2054\n",
    "#  sites.age_unit = \"Ma\"\n",
    "#  sites.dir_tilt_correction = 0 (geographc coordinates)\n",
    "#  sites.result_quality = \"g\" if n (col. W) is not nan, otherwise \"b\"\n",
    "\n",
    "letts_data = pd.read_excel(data_folder/'Letts_Bushveld.xlsx',sheet_name='Sheet1')\n",
    "#display(letts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile sites dataframe\n",
    "sites_df=pd.DataFrame([])\n",
    "sites_df['site']=letts_data['Full Site'].astype('string')\n",
    "#sites_df['location']=letts_data.apply(lambda x: \"{}:{}\".format(x['Region'],x['Site']),axis=1)\n",
    "sites_df['location']=letts_data['Region']\n",
    "sites_df['lat']=letts_data['DecLat']\n",
    "sites_df['lon']=letts_data['DecLong']\n",
    "sites_df['dir_n_total_samples']=letts_data['n']\n",
    "sites_df['dir_n_samples']=letts_data['n.1']\n",
    "sites_df['dir_dec']=letts_data['GDec']\n",
    "sites_df['dir_inc']=letts_data['GInc']\n",
    "sites_df['bed_dip']=letts_data['Dip']\n",
    "sites_df['bed_dip_direction']=(letts_data['Strike']+90.)%360.\n",
    "sites_df['dir_alpha95']=letts_data['a95.1']\n",
    "sites_df['dir_k']=letts_data['kappa']\n",
    "sites_df['lithologies']= letts_data['Lithology']\n",
    "sites_df['formation']= \"Bushveld Complex\"\n",
    "sites_df['geologic_classes']='Igneous:Intrusive'\n",
    "sites_df['result_type']= \"i\"\n",
    "sites_df['method_codes']= \"DE-BFL:DE-K:LP-DIR-AF:LP-DIR-T\"\n",
    "sites_df['citations']= \"10.1111/j.1365-246X.2009.04346.x\"\n",
    "sites_df['geologic_types']=\"Layered Intrusion\"\n",
    "sites_df['age']= 2054\n",
    "sites_df['age_unit']= \"Ma\"\n",
    "sites_df['dir_tilt_correction']=0\n",
    "sites_df['result_quality']= letts_data.apply(lambda x: 'b' if np.isnan(x['n.1']) else 'g',axis=1)\n",
    "#display(sites_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and attach VGPs\n",
    "# NOTE: Needs to be updated so that VGPs are calculated based on tilt-corrected data!\n",
    "vgps_df=add_vgps(sites_df)\n",
    "sites_df = pd.concat([sites_df,vgps_df],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many bootstraps do we need to determine kappa when averaging site-level data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Here we test the bootstrap to determine a \"total\" kappa value when averaging site means. We tested two groups of sites from Letts, rows 0-2 and 6-11. In both cases, the nb=500 bootstrap both provided a low enough variance (standard deviation of kappa from 7 runs is about 1% of the mean) and enough precision (average kappa of 7 runs is within 5% of the value at nb=10000) in a short enough time to make the bootstrap feasible._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dir_dec  dir_inc  dir_n_samples   dir_k\n",
      "0    146.2     86.3            9.0  271.37\n",
      "1    258.5     85.8            9.0  298.13\n",
      "2    126.6     89.1            9.0  537.21\n",
      "Fisher Mean:\n",
      "         dec       inc  n         r           k   alpha95      csd\n",
      "0  196.68653  88.45081  3  2.996202  526.577743  5.377664  3.52983\n",
      "Bootstrap (nb = 0):\n",
      "Bootstrap (nb = 10):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 32.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 20):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 29.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 50):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 30.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 50):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 31.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 50):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 29.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 50):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 29.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 50):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 30.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 50):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 29.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 50):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 31.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 100):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 29.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 200):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:07<00:00, 27.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 300):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 300/300 [00:09<00:00, 30.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 400):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [00:12<00:00, 31.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:16<00:00, 30.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 600):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [00:19<00:00, 31.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 700):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 700/700 [00:22<00:00, 31.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:16<00:00, 30.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:16<00:00, 29.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:16<00:00, 30.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:17<00:00, 29.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:16<00:00, 30.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:17<00:00, 29.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:16<00:00, 29.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 800):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 800/800 [00:27<00:00, 29.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 900):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [00:29<00:00, 30.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 1000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:32<00:00, 30.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 1000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:33<00:00, 29.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 1000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:33<00:00, 30.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 1000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:33<00:00, 30.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 1000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:33<00:00, 29.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 1000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:33<00:00, 30.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 1000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:34<00:00, 29.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap (nb = 10000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10000/10000 [05:55<00:00, 28.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      dir_dec    dir_inc dir_n       dir_k  dir_alpha95 result_type       nb\n",
      "0  196.686530  88.450810     3  526.577743     5.377664           a      0.0\n",
      "0  186.593726  88.646434     3  232.223334     8.119912           a     10.0\n",
      "0  202.680230  88.454499     3  225.458224     8.241757           a     20.0\n",
      "0  203.280784  88.335668     3  231.987705     8.124065           a     50.0\n",
      "0  193.527167  88.248484     3  234.713882     8.076397           a     50.0\n",
      "0  195.135119  88.345884     3  241.844279     7.955580           a     50.0\n",
      "0  199.216804  88.428298     3  252.989083     7.777148           a     50.0\n",
      "0  195.747683  88.338552     3  246.559736     7.878598           a     50.0\n",
      "0  196.593905  88.413653     3  227.181844     8.210193           a     50.0\n",
      "0  202.610557  88.483362     3  242.003718     7.952941           a     50.0\n",
      "0  196.168450  88.617119     3  238.196565     8.016704           a    100.0\n",
      "0  194.128296  88.387524     3  234.843247     8.074156           a    200.0\n",
      "0  195.389155  88.458563     3  235.942623     8.055184           a    300.0\n",
      "0  196.893652  88.469054     3  237.758835     8.024134           a    400.0\n",
      "0  197.576916  88.458515     3  233.592318     8.095906           a    500.0\n",
      "0  195.996549  88.511707     3  238.136230     8.017727           a    600.0\n",
      "0  197.822260  88.477806     3  238.488589     8.011759           a    700.0\n",
      "0  197.581941  88.432609     3  234.835822     8.074284           a    500.0\n",
      "0  196.911907  88.461961     3  235.635926     8.060463           a    500.0\n",
      "0  197.991890  88.450631     3  240.145784     7.983867           a    500.0\n",
      "0  196.123925  88.462150     3  234.218811     8.084991           a    500.0\n",
      "0  197.692518  88.409279     3  231.466931     8.133267           a    500.0\n",
      "0  197.176657  88.460561     3  235.753670     8.058435           a    500.0\n",
      "0  194.912590  88.396804     3  239.691358     7.991486           a    500.0\n",
      "0  196.311561  88.484268     3  239.380122     7.996717           a    800.0\n",
      "0  198.204093  88.436406     3  236.323211     8.048648           a    900.0\n",
      "0  195.476005  88.432104     3  238.257962     8.015664           a   1000.0\n",
      "0  196.970174  88.484144     3  235.124945     8.069282           a   1000.0\n",
      "0  196.453007  88.453184     3  233.926553     8.090077           a   1000.0\n",
      "0  197.622269  88.466341     3  233.633645     8.095185           a   1000.0\n",
      "0  197.061651  88.436003     3  235.407466     8.064403           a   1000.0\n",
      "0  196.436325  88.444889     3  238.091184     8.018491           a   1000.0\n",
      "0  196.109175  88.456033     3  234.216310     8.085034           a   1000.0\n",
      "0  196.665821  88.456546     3  237.064410     8.035964           a  10000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "to_avg=sites_df[sites_df['location']=='Main Zone Western Lobe'][['dir_dec','dir_inc','dir_n_samples','dir_k']].iloc[0:3,:]\n",
    "print(to_avg)\n",
    "print('Fisher Mean:')\n",
    "print(pd.DataFrame(pmag.dir_df_fisher_mean(to_avg),index=[0]))\n",
    "to_avg.columns=['dir_dec','dir_inc','dir_n','dir_k']\n",
    "boots=[0, 10, 20, 50, 50, 50, 50, 50, 50, 50, 100, 200, 300, 400, 500, 600, 700, 500, 500, 500, 500, 500, 500,\n",
    "       500, 800, 900, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 10000]\n",
    "results=pd.DataFrame([],columns=['dir_dec','dir_inc','dir_n','dir_k','dir_alpha95'])\n",
    "for nb in boots:\n",
    "    print(f'Bootstrap (nb = {nb}):')\n",
    "    result=dataframe_average(to_avg,nb=nb)\n",
    "    result['nb']=nb\n",
    "    results=results.append(result)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_averages = results.groupby('nb').apply(np.mean)\n",
    "result_std = results.groupby('nb').apply(np.std)\n",
    "print(result_std)\n",
    "big_kappa=result_averages.loc[10000,'dir_k']\n",
    "print(big_kappa)\n",
    "results['q']=100.*np.abs(results['dir_k']-big_kappa)/big_kappa\n",
    "print(result_averages)\n",
    "ax=plt.subplot(111)\n",
    "ax.plot(boots,100*np.abs(results['dir_k']-big_kappa)/big_kappa,'ko')\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(boots,results['dir_alpha95'],'bo')\n",
    "plt.title('Scree Plot')\n",
    "ax.set_xlabel('Number of bootstrap realizations')\n",
    "ax.set_ylabel('$100\\\\frac{\\kappa_i - \\\\kappa_{10000}}{\\\\kappa_{10000}}$')\n",
    "ax2.set_ylabel('$\\\\alpha_{95}$',color='blue')\n",
    "# Note: 500 is probably enough. Gives about 5% difference from 1000 average, std dev is ~1%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Back to our regularly-scheduled programming. If you are running this notebook to get MagIC data files, this is where the Letts (2009) processing resumes after testing the bootstraps._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In this block and the next, we create a \"locations.txt\" file by averaging sites for two different definitions of \"location\": \n",
    "#  Averaging by the \"location\" column (i.e. Main Zone Eastern Lobe, etc.)\n",
    "#  Averaging over the entire complex\n",
    "# These are initially created as two dataframes that are then merged in the next block.\n",
    "\n",
    "# Averaging by location:\n",
    "# Mapping names of columns. We will average site-level data. \n",
    "# Each site has a number of samples listed in the sites table, so we will use that as our N in the parametric bootstrap.\n",
    "sample_map={'dir_n_samples':'dir_n'} \n",
    "# Compile locations dataframe\n",
    "locations_df1=pd.DataFrame([])\n",
    "sites_group=tc_df.groupby('location')\n",
    "locations_df1['location']=sites_group.groups.keys()\n",
    "locations_df1['sites']=sites_group['site'].unique().apply(':'.join).reset_index(drop=True) # What are all the sites in this location?\n",
    "locations_df1['location_type']='Region'\n",
    "locations_df1['geologic_classes']='Igneous:Intrusive'\n",
    "locations_df1['lithologies']=sites_group['lithologies'].unique().apply(':'.join).reset_index(drop=True) # What are all the lithologies in this location?\n",
    "# The following 4 lines find the bounding box of coordinates for each grouping of sites. \n",
    "locations_df1['lat_s']=sites_group['lat'].min().reset_index(drop=True) \n",
    "locations_df1['lat_n']=sites_group['lat'].max().reset_index(drop=True)\n",
    "locations_df1['lon_e']=sites_group['lon'].max().reset_index(drop=True)\n",
    "locations_df1['lon_w']=sites_group['lon'].min().reset_index(drop=True)\n",
    "locations_df1['age']=2054\n",
    "locations_df1['age_unit']=\"Ma\"\n",
    "# The following does a parametric bootstrap for each location to find the average kappa and a95 values\n",
    "locations_df1=pd.concat([locations_df1,sites_group.apply(lambda x: dir_df_boot_ci(x,nb=10,column_map=sample_map)).reset_index()],axis=1)\n",
    "# Need to rename columns: now our \"n\" is numbers of sites, since locations_df is the locations dataframe (a site-level average).\n",
    "locations_df1=locations_df1.rename({'dir_n_samples':'dir_n_sites'})\n",
    "\n",
    "# Averaging for the whole complex:\n",
    "locations_df2=pd.DataFrame([])\n",
    "sites_group=tc_df.groupby('formation')\n",
    "locations_df2['location']=sites_group.groups.keys()\n",
    "locations_df2['sites']=sites_group['site'].unique().apply(':'.join).reset_index(drop=True)#.drop('formation',axis=1)\n",
    "locations_df2['location_type']='Region'\n",
    "locations_df2['geologic_classes']='Igneous:Intrusive'\n",
    "locations_df2['lithologies']=sites_group['lithologies'].unique().apply(':'.join).reset_index(drop=True)#.drop('formation',axis=1)\n",
    "locations_df2['lat_s']=sites_group['lat'].min().reset_index(drop=True)\n",
    "locations_df2['lat_n']=sites_group['lat'].max().reset_index(drop=True)\n",
    "locations_df2['lon_e']=sites_group['lon'].max().reset_index(drop=True)\n",
    "locations_df2['lon_w']=sites_group['lon'].min().reset_index(drop=True)\n",
    "locations_df2['age']=2054\n",
    "locations_df2['age_unit']=\"Ma\"\n",
    "locations_df2=pd.concat([locations_df2,sites_group.apply(lambda x: dir_df_boot_ci(x,nb=10,column_map=sample_map)).reset_index()],axis=1)\n",
    "locations_df2=locations_df2.rename(columns={'dir_n_samples':'dir_n_sites'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 1000, 'display.max_columns', 27):\n",
    "    display(tc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the two dataframes (with different definitions of what a \"location\" is)\n",
    "locations_df = pd.concat([locations_df1.iloc[:, [j for j, c in enumerate(locations_df1.columns) if j not in [11, 12, 18]]], \n",
    "                          locations_df2.iloc[:, [j for j, c in enumerate(locations_df2.columns) if j not in [11, 12, 18]]]],\n",
    "                         axis=0,ignore_index=True)\n",
    "display(locations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile contributions dataframe\n",
    "# Note that MagIC somehow does not recognize this when you upload it, so you have to specify the DOI and lab by hand anyway.\n",
    "contribution_df=pd.DataFrame({'reference':[\"10.1111/j.1365-246X.2009.04346.x\"],'lab_names':[\"Not Specified\"]},index=[0])\n",
    "print(contribution_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing dataframes to dictionaries for export\n",
    "contribution_dicts=contribution_df.fillna('').to_dict('records')\n",
    "locations_dicts=locations_df.fillna('').to_dict('records')\n",
    "sites_dicts = sites_df.fillna('').to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Write files for uploading to MagIC\n",
    "# You will still need to upload these to a private contribution by hand\n",
    "pmag.magic_write(magic_folder/'letts2009_contribution.txt', contribution_dicts, 'contribution')\n",
    "pmag.magic_write(str(magic_folder/'letts2009_locations.txt'), locations_dicts, 'locations')\n",
    "pmag.magic_write(magic_folder/'letts2009_sites.txt', sites_dicts, 'sites')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kosterov & Perrin (1996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Kosterov & Perrin (1996)\n",
    "#%%capture sites_conversion_log\n",
    "# Capture output to sites_conversion_log\n",
    "\n",
    "## Convert Sites XLSX Files to MagIC format\n",
    "\n",
    "# In this XLSX file, here is how I interpret the mapping of relevant columns to the MagIC 3.0 data model (PAS 10/07/2024): \n",
    "#  \n",
    "#\n",
    "#  Location: samples.site\n",
    "#  ID: samples.sample (had to add this column to remove ambiguities in site numbers)\n",
    "#  Lat: samples.lat\n",
    "#  Long: samples.lon\n",
    "#  N: samples.dir_n_total_specimens\n",
    "#  n: samples.dir_n_specimens\n",
    "#  Inc: samples.dir_inc\n",
    "#  Dec: samples.dir_dec\n",
    "#  k: samples.dir_k\n",
    "#  a95: samples.dir_alpha95\n",
    "#  GroupID: samples.site ()\n",
    "#  \n",
    "# Assumptions:\n",
    "#\n",
    "#  samples.result_type = \"i\"\n",
    "#  samples.method_codes = \"DE-BFL:DE-K:LP-DIR-AF:LP-DIR-T\"\n",
    "#  samples.citations = \"10.1016/0012-821X(96)00005-2\"\n",
    "#  samples.age = 180\n",
    "#  samples.age_sigma = 5\n",
    "#  samples.age_unit = \"Ma\"\n",
    "#  samples.dir_tilt_correction = 0\n",
    "#  samples.result_quality = \"g\" if group (col. N) is not nan, otherwise \"b\"\n",
    "\n",
    "kosterov1996_data = pd.read_excel(data_folder/'KarooData.xlsx',sheet_name='Kosterov 1996',skiprows=4).dropna(how='all')\n",
    "#display(kosterov1996_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile samples dataframe\n",
    "samples_df=pd.DataFrame([])\n",
    "samples_df['sample']=kosterov1996_data['ID'].astype('string')\n",
    "samples_df['site']=kosterov1996_data['GroupID'].astype('string')\n",
    "samples_df['site']=samples_df['site'].fillna('N/A')\n",
    "samples_df['location']=kosterov1996_data['Location']\n",
    "samples_df['lat']=kosterov1996_data['Lat']\n",
    "samples_df['lon']=kosterov1996_data['Long']\n",
    "samples_df['dir_n_total_specimens']=kosterov1996_data['N']\n",
    "samples_df['dir_n_specimens']=kosterov1996_data['n']\n",
    "samples_df['dir_dec']=kosterov1996_data['Dec']\n",
    "samples_df['dir_inc']=kosterov1996_data['Inc']\n",
    "samples_df['dir_alpha95']=kosterov1996_data['a95']\n",
    "samples_df['dir_k']=kosterov1996_data['k']\n",
    "samples_df['lithologies']= 'Basalt'\n",
    "samples_df['geologic_classes']='Igneous:Extrusive'\n",
    "samples_df['result_type']= \"i\"\n",
    "samples_df['method_codes']= \"DE-BFL:DE-K:LP-DIR-AF:LP-DIR-T\"\n",
    "samples_df['citations']= \"10.1016/0012-821X(96)00005-2\"\n",
    "samples_df['geologic_types']=\"Lava Flow\"\n",
    "samples_df['age']= 180\n",
    "samples_df['age_sigma']= 5\n",
    "samples_df['age_unit']= \"Ma\"\n",
    "samples_df['dir_tilt_correction']=0\n",
    "samples_df['result_quality']= 'g'\n",
    "samples_df.loc[samples_df['location'].isna(),'result_quality']='b'\n",
    "with pd.option_context('display.max_rows', 1000, 'display.max_columns', 10):\n",
    "    display(samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and attach VGPs\n",
    "vgps_df=add_vgps(samples_df)\n",
    "samples_df = pd.concat([samples_df,vgps_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_map={'dir_n_specimens':'dir_n'}\n",
    "# Compile sites dataframe\n",
    "sites_df=pd.DataFrame([])\n",
    "samples_group=samples_df.groupby('site')\n",
    "sites_df['site']=samples_group.groups.keys()\n",
    "sites_df['samples']=samples_group['sample'].unique().apply(':'.join).reset_index(drop=True)#.drop('site',axis=1)\n",
    "sites_df['site_type']='Outcrop'\n",
    "sites_df['geologic_classes']=samples_group['geologic_classes'].unique().apply(':'.join).reset_index(drop=True)#.drop('site',axis=1)\n",
    "sites_df['lithologies']='Basalt'\n",
    "sites_df['location']=samples_group.first()['location'].reset_index(drop=True)\n",
    "sites_df['lat_s']=samples_group['lat'].min().reset_index(drop=True)\n",
    "sites_df['lat_n']=samples_group['lat'].max().reset_index(drop=True)\n",
    "sites_df['lon_e']=samples_group['lon'].max().reset_index(drop=True)\n",
    "sites_df['lon_w']=samples_group['lon'].min().reset_index(drop=True)\n",
    "sites_df['age']=180\n",
    "sites_df['age_sigma']=5\n",
    "sites_df['age_unit']=\"Ma\"\n",
    "sites_df=pd.concat([sites_df,samples_group.apply(lambda x: dir_df_boot_ci(x,column_map=sample_map,nb=500)).reset_index()],axis=1)\n",
    "sites_df=sites_df.rename(columns={'dir_n_specimens':'dir_n_samples'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "location_map={'dir_n_samples':'dir_n'}\n",
    "# Compile locations dataframe\n",
    "locations_df=pd.DataFrame([])\n",
    "sites_group=sites_df.groupby('location')\n",
    "locations_df['location']=sites_group.groups.keys()\n",
    "locations_df['sites']=sites_group['location'].unique().apply(':'.join).reset_index(drop=True)\n",
    "locations_df['location_type']='Outcrop'\n",
    "locations_df['geologic_classes']=sites_group['geologic_classes'].unique().apply(':'.join).reset_index(drop=True)#.drop('location',axis=1)\n",
    "locations_df['lithologies']='Basalt'\n",
    "#locations_df['description']=sites_group.first()['description'].reset_index(drop=True)\n",
    "locations_df['lat_s']=sites_group['lat_s'].min().reset_index(drop=True)\n",
    "locations_df['lat_n']=sites_group['lat_n'].max().reset_index(drop=True)\n",
    "locations_df['lon_e']=sites_group['lon_e'].max().reset_index(drop=True)\n",
    "locations_df['lon_w']=sites_group['lon_w'].min().reset_index(drop=True)\n",
    "locations_df['age']=180\n",
    "locations_df['age_sigma']=5\n",
    "locations_df['age_unit']=\"Ma\"\n",
    "locations_df=pd.concat([locations_df,sites_group.apply(lambda x: dir_df_boot_ci(x,column_map=location_map)).reset_index()],axis=1)\n",
    "locations_df=locations_df.rename(columns={'dir_n_samples':'dir_n_sites'})\n",
    "with pd.option_context('display.max_rows', 1000, 'display.max_columns', 10):\n",
    "    display(locations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_dicts=locations_df.fillna('').to_dict('records')\n",
    "sites_dicts = sites_df.fillna('').to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pmag.magic_write(str(magic_folder/'kosterov1996_locations.txt'), locations_dicts, 'locations')\n",
    "pmag.magic_write(magic_folder/'kosterov1996_sites.txt', sites_dicts, 'sites')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moulin et al. 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Moulin et al. 2011\n",
    "#%%capture sites_conversion_log\n",
    "# Capture output to sites_conversion_log\n",
    "\n",
    "## Convert Sites XLSX Files to MagIC format\n",
    "\n",
    "# In this XLSX file, here is how I interpret the mapping of relevant columns to the MagIC 3.0 data model (PAS 8/17/2024): \n",
    "#  \n",
    "#\n",
    "#  Section: sites.description\n",
    "#  Site: sites.site (had to add this column to remove ambiguities in site numbers)\n",
    "#  Slat: sites.lat\n",
    "#  Slon: sites.lon\n",
    "#  N: sites.dir_n_total_samples\n",
    "#  n: sites.dir_n_samples\n",
    "#  Ig: sites.dir_inc\n",
    "#  Dg: sites.dir_dec\n",
    "#  k: sites.dir_k\n",
    "#  a95: sites.dir_alpha95\n",
    "#  Dir. Group or Flow: sites.location\n",
    "#  Age: sites.age\n",
    "#  dAge: sites.age_sigma\n",
    "#  \n",
    "# Assumptions:\n",
    "#\n",
    "#  sites.result_type = \"i\"\n",
    "#  sites.method_codes = \"DE-BFL:DE-K:LP-DIR-AF:LP-DIR-T\"\n",
    "#  sites.citations = \"10.1029/2011JB008210\"\n",
    "#  sites.age_unit = \"Ma\"\n",
    "#  sites.dir_tilt_correction = 0\n",
    "#  sites.result_quality = \"g\" if group is not nan, otherwise \"b\"\n",
    "\n",
    "moulin2011_data = pd.read_excel(data_folder/'KarooData.xlsx',sheet_name='Moulin 2011',skiprows=4,usecols=\"A:S\").dropna(how='all')\n",
    "#display(moulin2011_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile sites dataframe\n",
    "sites_df=pd.DataFrame([])\n",
    "sites_df['site']=moulin2011_data['Site'].astype('string')\n",
    "sites_df['formation']=moulin2011_data['Formation'].astype('string')\n",
    "sites_df['location']=moulin2011_data['Dir. Group or Flow'].astype('string')\n",
    "sites_df['description']=moulin2011_data['Section']\n",
    "sites_df['lat']=moulin2011_data['Slat']\n",
    "sites_df['lon']=moulin2011_data['Slon']\n",
    "sites_df['dir_n_total_samples']=moulin2011_data['N']\n",
    "sites_df['dir_n_samples']=moulin2011_data['n']\n",
    "sites_df['dir_dec']=moulin2011_data['Dg']\n",
    "sites_df['dir_inc']=moulin2011_data['Ig']\n",
    "sites_df['dir_alpha95']=moulin2011_data['a95']\n",
    "sites_df['dir_k']=moulin2011_data['k']\n",
    "sites_df['lithologies']= 'Basalt'\n",
    "sites_df['geologic_classes']='Igneous:Extrusive'\n",
    "sites_df['result_type']= \"i\"\n",
    "sites_df['method_codes']= \"DE-BFL:DE-K:LP-DIR-AF:LP-DIR-T\"\n",
    "sites_df['citations']= \"10.1029/2011JB008210\"\n",
    "sites_df['geologic_types']=\"Lava Flow\"\n",
    "sites_df['age']= moulin2011_data['Age']\n",
    "sites_df['age_sigma']= moulin2011_data['dAge']\n",
    "sites_df['age_unit']= \"Ma\"\n",
    "sites_df['dir_tilt_correction']=0\n",
    "sites_df['result_quality']= 'g'\n",
    "sites_df.loc[sites_df['location'].isna(),'result_quality']='b'\n",
    "with pd.option_context('display.max_rows', 1000, 'display.max_columns', 10):\n",
    "    display(sites_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and attach VGPs\n",
    "vgps_df=add_vgps(sites_df)\n",
    "sites_df = pd.concat([sites_df,vgps_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile locations dataframe\n",
    "locations_df1=pd.DataFrame([])\n",
    "sites_group=sites_df.groupby('location')\n",
    "locations_df1['location']=sites_group.groups.keys()\n",
    "locations_df1['sites']=sites_group['site'].unique().apply(':'.join).reset_index().drop('location',axis=1)\n",
    "locations_df1['description']=sites_group.first()['description'].reset_index(drop=True)\n",
    "locations_df1['location_type']='Outcrop'\n",
    "locations_df1['geologic_classes']=sites_group['geologic_classes'].unique().apply(':'.join).reset_index().drop('location',axis=1)\n",
    "locations_df1['lithologies']='Basalt'\n",
    "locations_df1['lat_s']=sites_df['lat'].min()\n",
    "locations_df1['lat_n']=sites_df['lat'].max()\n",
    "locations_df1['lon_e']=sites_df['lon'].max()\n",
    "locations_df1['lon_w']=sites_df['lon'].min()\n",
    "locations_df1['age_low']=sites_df['age'].min()\n",
    "locations_df1['age_high']=sites_df['age'].max()\n",
    "locations_df1['age_sigma']=sites_df['age_sigma'].max()\n",
    "locations_df1['age_unit']=\"Ma\"\n",
    "locations_df1=pd.concat([locations_df1,sites_group.apply(lambda x: fisher_avg(x)).reset_index()[['dir_dec','dir_inc','dir_n_sites','dir_k','dir_alpha95','result_type']]],\n",
    "                        axis=1)\n",
    "\n",
    "locations_df2=pd.DataFrame([])\n",
    "#sites_group=sites_df.groupby('description')\n",
    "sites_group=locations_df1.groupby('description')\n",
    "\n",
    "locations_df2['location']=sites_group.groups.keys()\n",
    "new_sites=sites_group.apply(lambda x:\":\".join(list(set(x['sites'].str.split(':').explode().tolist())))).reset_index()\n",
    "new_sites.columns=['location','sites']\n",
    "\n",
    "locations_df2=pd.merge(locations_df2,new_sites,on='location')\n",
    "locations_df2['location_type']='Stratigraphic Section'\n",
    "locations_df2['geologic_classes']=sites_group['geologic_classes'].unique().apply(':'.join).reset_index().drop('description',axis=1)\n",
    "locations_df2['lithologies']='Basalt'\n",
    "locations_df2['lat_s']=sites_df['lat'].min()\n",
    "locations_df2['lat_n']=sites_df['lat'].max()\n",
    "locations_df2['lon_e']=sites_df['lon'].max()\n",
    "locations_df2['lon_w']=sites_df['lon'].min()\n",
    "locations_df2['age_low']=sites_df['age'].min()\n",
    "locations_df2['age_high']=sites_df['age'].max()\n",
    "locations_df2['age_sigma']=sites_df['age_sigma'].max()\n",
    "locations_df2['age_unit']=\"Ma\"\n",
    "locations_df2=pd.concat([locations_df2,sites_group.apply(lambda x: fisher_avg(x)).reset_index()[['dir_dec','dir_inc','dir_n_sites','dir_k','dir_alpha95','result_type']]],\n",
    "                        axis=1)\n",
    "\n",
    "locations_df = pd.concat([locations_df1, locations_df2],axis=0).reset_index(drop=True)\n",
    "with pd.option_context('display.max_rows', 1000, 'display.max_columns', 10):\n",
    "    display(locations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_dicts=locations_df.fillna('').to_dict('records')\n",
    "sites_dicts = sites_df.fillna('').to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pmag.magic_write(str(magic_folder/'moulin2011_locations.txt'), locations_dicts, 'locations')\n",
    "pmag.magic_write(magic_folder/'moulin2011_sites.txt', sites_dicts, 'sites')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moulin et al. 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Moulin et al. 2012\n",
    "#%%capture sites_conversion_log\n",
    "# Capture output to sites_conversion_log\n",
    "\n",
    "## Convert Sites XLSX Files to MagIC format\n",
    "\n",
    "# In this XLSX file, here is how I interpret the mapping of relevant columns to the MagIC 3.0 data model (PAS 8/17/2024): \n",
    "#  \n",
    "#\n",
    "#  Section: sites.location\n",
    "#  Site: sites.site (had to add this column to remove ambiguities in site numbers)\n",
    "#  Lat: sites.lat\n",
    "#  Long: sites.lon\n",
    "#  N: sites.dir_n_total_samples\n",
    "#  n: sites.dir_n_samples\n",
    "#  Inc: sites.dir_inc\n",
    "#  Dec: sites.dir_dec\n",
    "#  k: sites.dir_k\n",
    "#  a95: sites.dir_alpha95\n",
    "#  Dir. Group: sites.group\n",
    "#  \n",
    "# Assumptions:\n",
    "#\n",
    "#  sites.result_type = \"i\"\n",
    "#  sites.method_codes = \"DE-BFL:DE-K:LP-DIR-AF:LP-DIR-T\"\n",
    "#  sites.citations = \"10.1029/2011GC003910\"\n",
    "#  sites.age = 179.2\n",
    "#  sites.age_sigma = 1.8\n",
    "#  sites.age_unit = \"Ma\"\n",
    "#  sites.dir_tilt_correction = 0\n",
    "#  sites.result_quality = \"g\" if group is not nan, otherwise \"b\"\n",
    "\n",
    "moulin2012_data = pd.read_excel(data_folder/'KarooData.xlsx',sheet_name='Moulin 2012',skiprows=3,usecols=\"A:L\").dropna(how='all')\n",
    "display(moulin2012_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile sites dataframe\n",
    "sites_df=pd.DataFrame([])\n",
    "sites_df['site']=moulin2012_data['Site'].astype('string')\n",
    "sites_df['groups']=moulin2012_data['Dir. Group'].astype('string')\n",
    "sites_df['location']=moulin2012_data['Section']\n",
    "sites_df['lat']=moulin2012_data['Lat']\n",
    "sites_df['lon']=moulin2012_data['Long']\n",
    "sites_df['dir_n_total_samples']=moulin2012_data['N']\n",
    "sites_df['dir_n_samples']=moulin2012_data['n']\n",
    "sites_df['dir_dec']=moulin2012_data['Dec']\n",
    "sites_df['dir_inc']=moulin2012_data['Inc']\n",
    "sites_df['dir_alpha95']=moulin2012_data['a95']\n",
    "sites_df['dir_k']=moulin2012_data['k']\n",
    "sites_df['lithologies']= 'Basalt'\n",
    "sites_df['geologic_classes']='Igneous:Extrusive'\n",
    "sites_df['result_type']= \"i\"\n",
    "sites_df['method_codes']= \"DE-BFL:DE-K:LP-DIR-AF:LP-DIR-T\"\n",
    "sites_df['citations']= \"10.1029/2011GC003910\"\n",
    "sites_df['geologic_types']=\"Lava Flow\"\n",
    "sites_df['age']= 179.2\n",
    "sites_df['age_sigma']= 1.8\n",
    "sites_df['age_unit']= \"Ma\"\n",
    "sites_df['dir_tilt_correction']=0\n",
    "sites_df['result_quality']= 'g'\n",
    "sites_df.loc[sites_df['groups'].isna(),'result_quality']='b'\n",
    "with pd.option_context('display.max_rows', 1000, 'display.max_columns', 10):\n",
    "    display(sites_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and attach VGPs\n",
    "vgps_df=add_vgps(sites_df)\n",
    "sites_df = pd.concat([sites_df,vgps_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile locations dataframe\n",
    "locations_df=pd.DataFrame([])\n",
    "sites_group=sites_df.groupby('location')\n",
    "locations_df['location']=sites_group.groups.keys()\n",
    "locations_df['sites']=sites_group['site'].unique().apply(':'.join).reset_index().drop('location',axis=1)\n",
    "locations_df['location_type']='Stratigraphic Section'\n",
    "locations_df['geologic_classes']=sites_group['geologic_classes'].unique().apply(':'.join).reset_index().drop('location',axis=1)\n",
    "locations_df['lithologies']='Basalt'\n",
    "locations_df['lat_s']=sites_df['lat'].min()\n",
    "locations_df['lat_n']=sites_df['lat'].max()\n",
    "locations_df['lon_e']=sites_df['lon'].max()\n",
    "locations_df['lon_w']=sites_df['lon'].min()\n",
    "locations_df['age']=179.2\n",
    "locations_df['age_sigma']=1.8\n",
    "locations_df['age_unit']=\"Ma\"\n",
    "with pd.option_context('display.max_rows', 1000, 'display.max_columns', 10):\n",
    "    display(locations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_dicts=locations_df.to_dict('records')\n",
    "sites_dicts = sites_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pmag.magic_write(str(magic_folder/'moulin2012_locations.txt'), locations_dicts, 'locations')\n",
    "pmag.magic_write(magic_folder/'moulin2012_sites.txt', sites_dicts, 'sites')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moulin et al. 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Moulin et al. 2017\n",
    "#%%capture sites_conversion_log\n",
    "# Capture output to sites_conversion_log\n",
    "\n",
    "## Convert Sites XLSX Files to MagIC format\n",
    "\n",
    "# In this XLSX file, here is how I interpret the mapping of relevant columns to the MagIC 3.0 data model (PAS 8/17/2024): \n",
    "#  \n",
    "#\n",
    "#  Section: sites.location\n",
    "#  Site: sites.site (had to add this column to remove ambiguities in site numbers)\n",
    "#  Slat (deg): sites.lat\n",
    "#  Slon (deg): sites.lon\n",
    "#  N: sites.dir_n_total_samples\n",
    "#  n: sites.dir_n_samples\n",
    "#  Ig (deg): sites.dir_inc\n",
    "#  Dg (deg): sites.dir_dec\n",
    "#  K: sites.dir_k\n",
    "#  a95 (deg): sites.dir_alpha95\n",
    "#  Directional Group or Single Flow: sites.group\n",
    "#  \n",
    "# Assumptions:\n",
    "#\n",
    "#  sites.result_type = \"i\"\n",
    "#  sites.method_codes = \"DE-BFL:DE-K:LP-DIR-AF:LP-DIR-T\"\n",
    "#  sites.citations = \"10.1002/2016JB013354\"\n",
    "#  sites.age = 181.1\n",
    "#  sites.age_sigma = 1.0\n",
    "#  sites.age_unit = \"Ma\"\n",
    "#  sites.dir_tilt_correction = 0\n",
    "#  sites.result_quality = \"g\" if group is not nan, otherwise \"b\"\n",
    "\n",
    "moulin2017_data = pd.read_excel(data_folder/'KarooData.xlsx',sheet_name='Moulin 2017',skiprows=1,usecols=\"A:L\").dropna(how='all')\n",
    "display(moulin2017_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile sites dataframe\n",
    "sites_df=pd.DataFrame([])\n",
    "sites_df['site']=moulin2017_data['Site'].astype('string')\n",
    "sites_df['groups']=moulin2017_data['Directional Group or Single Flow'].astype('string')\n",
    "sites_df['location']=moulin2017_data['Section']\n",
    "sites_df['lat']=moulin2017_data['Slat (deg)']\n",
    "sites_df['lon']=moulin2017_data['Slon (deg)']\n",
    "sites_df['dir_n_total_samples']=moulin2017_data['N']\n",
    "sites_df['dir_n_samples']=moulin2017_data['n']\n",
    "sites_df['dir_dec']=moulin2017_data['Dg (deg)']\n",
    "sites_df['dir_inc']=moulin2017_data['Ig (deg)']\n",
    "sites_df['dir_alpha95']=moulin2017_data['a95 (deg)']\n",
    "sites_df['dir_k']=moulin2017_data['K']\n",
    "sites_df['lithologies']= 'Basalt'\n",
    "sites_df['geologic_classes']='Igneous:Extrusive'\n",
    "sites_df['result_type']= \"i\"\n",
    "sites_df['method_codes']= \"DE-BFL:DE-K:LP-DIR-AF:LP-DIR-T\"\n",
    "sites_df['citations']= \"10.1002/2016JB013354\"\n",
    "sites_df['geologic_types']=\"Lava Flow\"\n",
    "sites_df['age']= 181.1\n",
    "sites_df['age_sigma']= 1.0\n",
    "sites_df['age_unit']= \"Ma\"\n",
    "sites_df['dir_tilt_correction']=0\n",
    "sites_df['result_quality']= 'g'\n",
    "sites_df.loc[sites_df['groups'].isna(),'result_quality']='b'\n",
    "with pd.option_context('display.max_rows', 1000, 'display.max_columns', 10):\n",
    "    display(sites_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and attach VGPs\n",
    "vgps_df=add_vgps(sites_df)\n",
    "sites_df = pd.concat([sites_df,vgps_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile locations dataframe\n",
    "locations_df=pd.DataFrame([])\n",
    "sites_group=sites_df.groupby('location')\n",
    "locations_df['location']=sites_group.groups.keys()\n",
    "locations_df['sites']=sites_group['site'].unique().apply(':'.join).reset_index().drop('location',axis=1)\n",
    "locations_df['location_type']='Stratigraphic Section'\n",
    "locations_df['geologic_classes']=sites_group['geologic_classes'].unique().apply(':'.join).reset_index().drop('location',axis=1)\n",
    "locations_df['lithologies']='Basalt'\n",
    "locations_df['lat_s']=sites_df['lat'].min()\n",
    "locations_df['lat_n']=sites_df['lat'].max()\n",
    "locations_df['lon_e']=sites_df['lon'].max()\n",
    "locations_df['lon_w']=sites_df['lon'].min()\n",
    "locations_df['age']=181.1\n",
    "locations_df['age_sigma']=1.0\n",
    "locations_df['age_unit']=\"Ma\"\n",
    "with pd.option_context('display.max_rows', 1000, 'display.max_columns', 10):\n",
    "    display(locations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_dicts=locations_df.to_dict('records')\n",
    "sites_dicts = sites_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pmag.magic_write(str(magic_folder/'moulin2017_locations.txt'), locations_dicts, 'locations')\n",
    "pmag.magic_write(magic_folder/'moulin2017_sites.txt', sites_dicts, 'sites')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pmagpy_env)",
   "language": "python",
   "name": "pmagpy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
